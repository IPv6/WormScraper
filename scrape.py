import bs4 as bs
import urllib.request
import sys
import os

#Dictionary to store sitemap.
ScraperURL = []

#Reads which web series to scrape.
option = sys.argv[1]

#Runs the prerequisite python file which builds the sitemap.
#This can be done manually if needed.
os.system("python " + option + ".py")

#Reads the sitemap data generated by the previous system call.
with open(option + '-sitemap.txt') as f:

#Loads data into dictionary and splits it by new line
#Each entry in the dictionary is now a seperate URL to be scraped.
    ScraperURL = f.read()
    ScraperURL = ScraperURL.split("\n")

#Opens the file the output is to be written in.
file = open(option + ".txt","w",encoding="utf-8")

#Run scraper for every entry in the dictionary.
for url in ScraperURL:
    SourceURL = url

#Open the url and read the source.
    source = urllib.request.urlopen(SourceURL).read()
    soup = bs.BeautifulSoup(source,'lxml')

#Prints current URL being scraped for debugging convenience.
    print("Starting "+ SourceURL +"\n")

#Adds title of each chapter at top.
    file.write(soup.title.text)
    file.write("\n\n\n\n")

#For every single p tag, the text within is scraped and added to the current output file.
    for paragraph in soup.find_all('p'):
#Some p tags are used in the source site for paragraph breaks. Ignores those.
        if(paragraph.string!=None):

#Writes the text to output file.
            paragh = (paragraph.string)
            file.write(paragh)
#I prefer two line breaks to indicate a paragraph break. This can be changed as required.
            file.write("\n\n")
        else:
            continue
#End of chapter break.
    file.write("\n\n\n\n")
#End of chapter shows in console output for user convenience.
    print("Done with Chapter\n")
print("All done with " + option + ". Use Calibre to convert into preferred format. :]\n")
file.close()
